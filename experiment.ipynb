{"cells":[{"cell_type":"markdown","metadata":{},"source":["Data and Project source: https://www.kaggle.com/competitions/gan-getting-started/overview"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["photo_path = 'picashu/data/photo_jpg'\n","monet_path = 'picashu/data/monet_jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dataset class:\n","\n","class Images(Dataset):\n","    def __init__(self, photo_path, monet_path, transform):\n","        self.photo_path = photo_path\n","        self.monet_path = monet_path\n","        self.transform = transform\n","        self.photos = os.listdir(photo_path)\n","        self.monets = os.listdir(monet_path)\n","        self.l_photo = len(self.photos)\n","        self.l_monet = len(self.monets)\n","    \n","    def __len__(self):\n","        return max(len(self.photos), len(self.monets))\n","    \n","    def __getitem__(self, idx):\n","        photo = Image.open(self.photo_path + self.photos[idx % self.l_photo]).convert(\"RGB\")\n","        monet = Image.open(self.monet_path + self.monets[idx % self.l_monet]).convert(\"RGB\")\n","        \n","        photo = self.transform(photo)\n","        monet = self.transform(monet)\n","        \n","        return photo, monet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define dataset:\n","\n","dataset = Images(photo_path, monet_path, transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define torch dataloader:\n","\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check examples of pics:\n","example = next(iter(dataloader))\n","\n","plt.subplot(1, 2, 1)\n","plt.title('Photo example')\n","plt.imshow(example[0][0].permute(1, 2, 0) * 0.5 + 0.5)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Monet example')\n","plt.imshow(example[1][0].permute(1, 2, 0) * 0.5 + 0.5)"]},{"cell_type":"markdown","metadata":{},"source":["Discriminator model"]},{"cell_type":"markdown","metadata":{},"source":["1. conv_block Class:\n","\n","This class defines a reusable convolutional block.\n","\n","Purpose: To create a series of convolutional layers followed by normalization and activation, which are commonly used in GANs to downsample the input image and extract features.\n","\n","Components:\n","__init__ Method:\n","\n","nn.Conv2d: Applies a 2D convolution to the input image. The parameters include:\n","in_channels: Number of input channels (e.g., 3 for RGB images).\n","out_channels: Number of filters used in the convolution, which determines the output channels.\n","kernel_size=4: A 4x4 filter size.\n","stride: Controls the step size of the convolution. A stride of 2 halves the image size, while 1 retains it.\n","padding=1: Adds padding to the input to maintain the image's spatial size.\n","padding_mode=\"reflect\": Uses reflection padding to avoid boundary artifacts.\n","bias=True: Adds a bias term.\n","nn.InstanceNorm2d: Normalizes the activations across each channel to stabilize training.\n","nn.LeakyReLU(0.2): A leaky ReLU activation function with a slope of 0.2 for negative values, allowing a small gradient to pass through.\n","\n","forward Method:\n","\n","This method defines how data flows through the layers. It takes an input tensor x and applies the sequential operations defined in __init__."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class conv_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels,\n","                      out_channels=out_channels, \n","                      kernel_size=4, \n","                      stride=stride,\n","                      padding=1,\n","                      bias=True,\n","                      padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.LeakyReLU(0.2)\n","        )\n","    \n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"markdown","metadata":{},"source":["2. Discriminator Class:\n","\n","This class defines the architecture of the discriminator model, which classifies whether an input image is real or fake.\n","\n","Purpose: In GANs, the discriminator learns to distinguish real images from fake ones generated by the generator.\n","\n","\n","Components:\n","__init__ Method:\n","\n","self.initial: The first layer of the discriminator:\n","nn.Conv2d: A 2D convolution similar to the one in conv_block, with in_channels=3 for RGB images, out_channels=64, and a stride of 2 to downsample the image.\n","nn.LeakyReLU(0.2): Activation after the initial convolution.\n","self.process: The main sequence of layers after the initial one:\n","Several conv_blocks, each progressively downsampling and increasing the number of filters:\n","conv_block(64, 128, 2): From 64 to 128 channels, halving the spatial dimensions.\n","conv_block(128, 256, 2): From 128 to 256 channels.\n","conv_block(256, 512, 1): From 256 to 512 channels but with no spatial reduction due to stride=1.\n","A final convolution layer nn.Conv2d(512, 1, kernel_size=4, stride=1) to reduce the feature map to a single output channel.\n","nn.Sigmoid(): Outputs a probability value between 0 and 1 (fake or real).\n","\n","forward Method:\n","\n","This method defines the forward pass of the discriminator.\n","\n","The input tensor passes through the initial layer and then through the layers in process. The final output is a tensor that indicates whether the input is real or fake.\n","\n","\n","Output Shape:\n","The input is assumed to have the shape [batch_size, 3, 256, 256] (e.g., a batch of RGB images with a resolution of 256x256).\n","The output tensor has a shape of [batch_size, 1, 30, 30], meaning that the model generates a 30x30 grid of \"real/fake\" predictions for different patches of the input image."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=in_channels,\n","                out_channels=64,\n","                kernel_size=4, \n","                stride=2,\n","                padding=1,\n","                padding_mode=\"reflect\"),\n","            nn.LeakyReLU(0.2)\n","        )\n","        self.process = nn.Sequential(\n","            conv_block(64, 128, 2),\n","            conv_block(128, 256, 2),\n","            conv_block(256, 512, 1),\n","            nn.Conv2d(\n","                in_channels=512,\n","                out_channels=1,\n","                kernel_size=4,\n","                stride=1,\n","                padding=1,\n","                padding_mode='reflect'),\n","            nn.Sigmoid()\n","        )\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        OUT = floor((IN + 2 * padding - kernel_size + 1) / stride + 1)\n","        [batch_size, 3, 256, 256] ->\n","        [batch_size, 64, 128, 128] ->\n","        [batch_size, 128, 64, 64] ->\n","        [batch_size, 256, 32, 32] ->\n","        [batch_size, 512, 30, 30] ->\n","        [batch_size, 1, 30, 30]\n","        \"\"\"\n","        x = self.initial(x)\n","        x = self.process(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = torch.randn((1, 3, 256, 256))\n","dis = Discriminator()\n","assert(dis(x).shape == (1, 1, 30, 30))"]},{"cell_type":"markdown","metadata":{},"source":["Generator Model"]},{"cell_type":"markdown","metadata":{},"source":["1. gen_conv_block Class:\n","\n","This class defines a convolutional block used for both downsampling (convolution) and upsampling (transpose convolution).\n","\n","Components:\n","\n","__init__ Method:\n","\n","in_channels and out_channels: The number of input and output channels for the block.\n","TYPE: Determines whether the block performs downsampling ('down' for convolution) or upsampling ('up' for transposed convolution).\n","activation: A flag indicating whether to apply an activation function (ReLU) after normalization.\n","Downsampling: If TYPE == 'down', a nn.Conv2d layer is used to reduce the spatial dimensions of the input.\n","Upsampling: If TYPE == 'up', a nn.ConvTranspose2d layer is used to increase the spatial dimensions.\n","Instance Normalization: nn.InstanceNorm2d(out_channels) normalizes the activations across each channel.\n","Activation: nn.ReLU(inplace=True) if activation=True, otherwise nn.Identity(), which means no activation.\n","\n","\n","forward Method:\n","Defines the forward pass, applying the convolution or transposed convolution, normalization, and optionally the activation function."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class gen_conv_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, TYPE='down', activation=False, **kwargs):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels,\n","                      out_channels=out_channels,\n","                      padding_mode=\"reflect\",\n","                      **kwargs) if TYPE == 'down'\n","            else nn.ConvTranspose2d(in_channels=in_channels,\n","                                   out_channels=out_channels,\n","                                   **kwargs),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.ReLU(inplace=True) if activation else nn.Identity()\n","        )\n","        \n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"markdown","metadata":{},"source":["2. res_block Class:\n","This class defines a residual block, which is commonly used in ResNet architectures. Residual blocks help with training deep networks by allowing gradients to pass through more easily.\n","\n","Components:\n","__init__ Method:\n","Two convolutional blocks (using gen_conv_block) with the same number of channels. These blocks operate with a kernel_size=3 and padding=1 to preserve spatial dimensions.\n","The second block does not use an activation function (activation=False).\n","\n","forward Method:\n","Applies the residual connection: x + self.block_(x). This adds the input x to the output of the two convolutional layers (self.block_), allowing gradients to propagate more easily through the network."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class res_block(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.block_ = nn.Sequential(\n","            gen_conv_block(channels, channels, kernel_size=3, padding=1),\n","            gen_conv_block(channels, channels, activation=False, kernel_size=3, padding=1)\n","        )\n","    \n","    def forward(self, x):\n","        return x + self.block_(x)"]},{"cell_type":"markdown","metadata":{},"source":["3. Generator Class:\n","This class defines the Generator architecture, responsible for producing an output image from an input image.\n","\n","Components:\n","\n","__init__ Method:\n","Initial Convolution:\n","nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"): A 7x7 convolution that preserves spatial dimensions (thanks to padding=3) and increases the number of channels from in_channels (usually 3 for RGB images) to 64.\n","nn.ReLU(inplace=True): Applies ReLU activation.\n","\n","Downsampling:\n","self.down: This uses two gen_conv_blocks to downsample the input image while increasing the number of channels. The spatial dimensions are halved at each step (stride=2), while the number of channels increases:\n","First gen_conv_block(64, 64*2): Reduces spatial dimensions and increases channels to 128.\n","Second gen_conv_block(64*2, 64*4): Reduces dimensions further and increases channels to 256.\n","\n","Residual Blocks:\n","self.residual: This contains a sequence of residual blocks (the res_block class). The number of residual blocks is defined by num_residuals_blocks, which is 9 in this case.\n","Each res_block keeps the spatial dimensions and number of channels constant (256 channels).\n","\n","\n","Upsampling:\n","self.up: This contains two gen_conv_blocks that perform upsampling (with TYPE='up'):\n","First gen_conv_block(64*4, 64*2): Doubles the spatial dimensions and reduces the number of channels from 256 to 128.\n","Second gen_conv_block(64*2, 64): Doubles the spatial dimensions again and reduces the number of channels from 128 to 64.\n","\n","\n","Final Image Generation:\n","self.get_img: This final convolution converts the output of the upsampling blocks back to the original number of channels (in_channels, usually 3 for RGB images).\n","The kernel size of 7x7 is used again, and padding=3 ensures that the spatial dimensions of the output match the input.\n","\n","\n","forward Method:\n","The input x goes through the following stages:\n","Initial Convolution (self.initial).\n","Downsampling (self.down): Reduces spatial dimensions and increases channels.\n","Residual Blocks (self.residual): Processes the features using 9 residual blocks.\n","Upsampling (self.up): Increases spatial dimensions and decreases channels.\n","Image Generation (self.get_img): Converts the processed feature map back into an image with the original number of channels.\n","\n","\n","\n","Summary of the Flow:\n","Input Image → Initial convolution → Downsampling → Residual blocks → Upsampling → Generated Image.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, in_channels=3, num_residuals_blocks=9):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.down = nn.Sequential(\n","            gen_conv_block(64, 64*2, TYPE='down', kernel_size=3, stride=2, padding=1),\n","            gen_conv_block(64*2, 64*4, TYPE='down', kernel_size=3, stride=2, padding=1)\n","        )\n","        self.residual = nn.Sequential(\n","            *[res_block(64*4) for _ in range(num_residuals_blocks)]\n","        )\n","        self.up = nn.Sequential(\n","            gen_conv_block(64*4, 64*2, TYPE='up', kernel_size=3, stride=2, padding=1, output_padding=1),\n","            gen_conv_block(64*2, 64, TYPE='up', kernel_size=3, stride=2, padding=1, output_padding=1)\n","        )\n","        self.get_img = nn.Conv2d(64, in_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n","        \n","    def forward(self, x):\n","        x = self.initial(x)\n","        x = self.down(x)\n","        x = self.residual(x)\n","        x = self.up(x)\n","        return self.get_img(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = torch.randn((1, 3, 256, 256))\n","dis = Generator()\n","assert(dis(x).shape == (1, 3, 256, 256))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr = 2e-4\n","lambda_cycle = 10"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["disc_photo = Discriminator().to(device)\n","disc_monet = Discriminator().to(device)\n","\n","gen_photo = Generator().to(device)\n","gen_monet = Generator().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["disc_optimizer = optim.Adam(\n","    list(disc_photo.parameters()) + list(disc_monet.parameters()),\n","    lr=lr,\n","    betas=(0.5, 0.999)\n",")\n","\n","gen_optimizer = optim.Adam(\n","    list(gen_photo.parameters()) + list(gen_monet.parameters()),\n","    lr=lr,\n","    betas=(0.5, 0.999)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dis_scaler = torch.amp.GradScaler('cuda')\n","gen_scaler = torch.amp.GradScaler('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MSE = nn.MSELoss()\n","L1 = nn.L1Loss()"]},{"cell_type":"markdown","metadata":{},"source":["Training Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epoches = 1\n","\n","for epoch in range(epoches):\n","    running_dis_loss = 0.0\n","    running_gen_loss = 0.0\n","    for photo, monet in tqdm(dataloader, leave=True):\n","        photo = photo.to(device)\n","        monet = monet.to(device)\n","        \n","        # Train discriminator:\n","        fake_photo = gen_photo(monet)\n","        Dis_photo_real = disc_photo(photo)\n","        Dis_photo_fake = disc_photo(fake_photo.detach())\n","        \n","        Dis_photo_loss = MSE(Dis_photo_real, torch.ones_like(Dis_photo_real)) + \\\n","                         MSE(Dis_photo_fake, torch.zeros_like(Dis_photo_fake))\n","        \n","        fake_monet = gen_monet(photo)\n","        Dis_monet_real = disc_monet(monet)\n","        Dis_monet_fake = disc_monet(fake_monet.detach())\n","        \n","        Dis_monet_loss = MSE(Dis_monet_real, torch.ones_like(Dis_monet_real)) + \\\n","                         MSE(Dis_monet_fake, torch.zeros_like(Dis_monet_fake))\n","        \n","        Dis_loss = (Dis_photo_loss + Dis_monet_loss) / 2.0\n","        running_dis_loss += Dis_loss / len(dataloader)\n","        \n","        disc_optimizer.zero_grad()\n","        dis_scaler.scale(Dis_loss).backward()\n","        dis_scaler.step(disc_optimizer)\n","        dis_scaler.update()\n","        \n","        # Train Generator:\n","        Dis_photo_fake = disc_photo(fake_photo)\n","        Dis_monet_fake = disc_monet(fake_monet)\n","        \n","        Gen_photo_loss = MSE(Dis_photo_fake, torch.ones_like(Dis_photo_fake))\n","        Gen_monet_loss = MSE(Dis_monet_fake, torch.ones_like(Dis_monet_fake))\n","        \n","        Cycled_monet = gen_monet(fake_photo) \n","        Cycled_photo = gen_photo(fake_monet)\n","        \n","        Cycled_loss = L1(monet, Cycled_monet) + L1(photo, Cycled_photo)\n","        \n","        Gen_loss = Gen_photo_loss + Gen_monet_loss + Cycled_loss * lambda_cycle\n","        running_gen_loss += Gen_loss / len(dataloader)\n","        \n","        gen_optimizer.zero_grad()\n","        gen_scaler.scale(Gen_loss).backward()\n","        gen_scaler.step(gen_optimizer)\n","        gen_scaler.update()\n","    print(f\"Epoch {epoch + 1}. Generator loss by epoch: {running_gen_loss}, discriminator loss by epoch: {running_dis_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(disc_photo.state_dict(), '/kaggle/working/disc_photo.pth')\n","torch.save(disc_monet.state_dict(), '/kaggle/working/disc_monet.pth')\n","torch.save(gen_photo.state_dict(), '/kaggle/working/gen_photo.pth')\n","torch.save(gen_monet.state_dict(), '/kaggle/working/gen_monet.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch = next(iter(dataloader))[0]\n","\n","_, ax = plt.subplots(5, 2, figsize=(12, 12))\n","\n","for i in range(5):\n","    original_img = batch[i]\n","    predicted_img = None\n","    with torch.no_grad():\n","        predicted_img = gen_monet(original_img.unsqueeze(0).to(device))\n","    \n","    ax[i, 0].imshow(original_img.permute(1, 2, 0) * 0.5 + 0.5)\n","    ax[i, 1].imshow(predicted_img.squeeze(0).permute(1, 2, 0).cpu() * 0.5 + 0.5)\n","    \n","    ax[i, 0].set_title(\"Original photo\")\n","    ax[i, 1].set_title(\"Monet like\")\n","    \n","    ax[i, 0].axis(\"off\")\n","    ax[i, 1].axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["This project is inspired from https://github.com/junyanz/CycleGAN"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
